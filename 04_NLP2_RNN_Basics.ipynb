{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJkPDd5vX1/xpSacoH8+z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annesjyu/NLP2/blob/main/04_NLP2_RNN_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence Prediction\n",
        "\n",
        "A sequence is an ordered collection of items. Traditional mechine learning algorithms assume data items to be independent and identically distributed (IID), but language task uses tokens which depend on the tokens before and after them. For example,\n",
        "\n",
        "<`Two dogs`> <`are`> barking at the little boy.\n",
        "\n",
        "In which the verb must agree with the number of subjects.\n",
        "\n",
        "Sometimes there dependencies can be arbitrarily long. For example,\n",
        "\n",
        "The <`dog`> has a short, yet luxuriously puffy <`tail`>."
      ],
      "metadata": {
        "id": "r3lFUWqNU8Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hidden State\n",
        "\n",
        "It is a special intermediate vector generated from the previous tokens, which carries the context information seen so far. It can be used with the current input token to predict the next token.\n",
        "\n",
        "The image below shows a character-level model taking hidden states together with input characters.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/annesjyu/NLP2/fc8c2ed7d01030820709417e4a879ca4738fd687/rnn-train.svg\" width=450></img>\n",
        "\n",
        "<small> *Image credit to [ Recurrent Neural Networks. d2i.ai](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html).</small>"
      ],
      "metadata": {
        "id": "g_4BMFSkZV6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "RNNs are a family of models following a recurrent architecture, which takes hidden states as well as current input to predict the next output. Here, we are going to introduct the simplest RNN, Elman RNN."
      ],
      "metadata": {
        "id": "SJY-eLMXfBrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elman RNN\n",
        "\n",
        "It is the RNN introducing the concept of internal memory into neural networks. The memory stores internal states based on previous inputs. It is designed to process sequences of data, for example, time series, sentences.\n",
        "\n",
        "It is composed of 3 layers:\n",
        "\n",
        "1. Input layer, is the input sequence.\n",
        "2. `Hidden layer`, processes the inputs by taking hidden state computed from previous input and hidden state at that old time point. The recurrence forms the memory aspect of the network.\n",
        "3. Output layer, produces the final output.\n",
        "\n",
        "The network is also known as `backpropagation through time (BPTT)`, which is a variant of the standard backpropagation algorithm used to train RNNs by adapting to handle the `temporal` dimension.\n",
        "\n",
        "<img src=\"https://github.com/annesjyu/NLP2/blob/main/rnn-2.gif?raw=true\" width=350></img>\n",
        "<small>* Loye, G. (2023, January 23). Beginner’s Guide on Recurrent Neural Networks with PyTorch. FloydHub Blog. https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/.</small>"
      ],
      "metadata": {
        "id": "M4cRNb-Fm2j1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathematical Representation\n",
        "\n",
        "Given an input of sequence items, $x_1, x_2, ..., x_T$, for each time step $t$, the hidden state $h_t$, and the output $y_t$ are computed as,\n",
        "\n",
        "$\n",
        "h_t = f(U*x_t + W * h_{t-1} + b), \\\\\n",
        "y_t = V * h_t + c\n",
        "$\n",
        "\n",
        "Where,\n",
        "* $f$ is an activation function, e.g. ReLU.\n",
        "* $U$, $W$, and $V$ are the weight matrices for input, hidden, and output.\n",
        "* $b$ and $c$ are bias vectors.\n",
        "* $h_{t-1}$ is the hidden state from the previous time step.\n"
      ],
      "metadata": {
        "id": "SdDEeQLYr7hD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "q51pugM7tQzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "vSPruIfQXQd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_size,\n",
        "               hidden_size,\n",
        "               output_size,\n",
        "               num_layers=1):\n",
        "    '''\n",
        "    input_size: The number of expected features in the input x\n",
        "    hidden_size: The number of features in the hidden state h\n",
        "    output_size: The number of features in the output y\n",
        "    num_layers: The number of recurrent layers.\n",
        "    '''\n",
        "    super(SimpleRNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    # The RNN layer.\n",
        "    # batch_first: If True, then the input and output tensors are provided as\n",
        "    #  (batch, seq, feature), instead of (seq, batch, feature). Note that this\n",
        "    #  does not apply to hidden states or cell states. Default: False.\n",
        "    self.rnn = nn.RNN(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=num_layers,\n",
        "                      batch_first=True, # the first batch\n",
        "                      bias=True,\n",
        "                      nonlinearity='tanh') # can also use relu\n",
        "    # Go through another linear layer to compute final output.\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Initialize the hidden state with zeros.\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "    hidden, _ = self.rnn(x, h0)\n",
        "    # Pass the output of the last time step to the final classifier\n",
        "    return self.fc(hidden[:, -1, :])"
      ],
      "metadata": {
        "id": "S4OGLC1ZWOd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the simple RNN"
      ],
      "metadata": {
        "id": "FfhqgVMxPV9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import randn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "c_pnIewEr4am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_size = 10\n",
        "sequence_width = 7\n",
        "num_classes = 3\n",
        "\n",
        "model1 = SimpleRNN(input_size=one_hot_size,\n",
        "                   hidden_size=32,\n",
        "                   output_size=num_classes,\n",
        "                   num_layers=2)\n",
        "print(model1)"
      ],
      "metadata": {
        "id": "J1IG9FXO-Kty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainRandn(model,\n",
        "               n_epochs=5,\n",
        "               n_batches=128,\n",
        "               batch_size=3,\n",
        "               sequence_width=7,\n",
        "               one_hot_size=10,\n",
        "               num_classes=3):\n",
        "  '''\n",
        "    Train the model with the given parameters using randomly generated data. The\n",
        "    total size of input is batch_size * n_batches, with each input having a dim\n",
        "    of sequence_width * one_hot_size.\n",
        "  '''\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in range(n_batches):\n",
        "        # Generate fake data for the batch.\n",
        "        x = torch.randn(batch_size, sequence_width, one_hot_size)\n",
        "        y_target = torch.randn(batch_size, num_classes)\n",
        "        #print(f'x={x}')\n",
        "        #print(f'y_target={y_target}')\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        y_pred = model(x)\n",
        "        #print(f'y_pred={y_pred}')\n",
        "        #print(f'y_pred.shape={y_pred.shape}')\n",
        "        #print(f'y_target.shape={y_target.shape}')\n",
        "\n",
        "        # Compute and print loss.\n",
        "        loss = loss_fn(y_pred, y_target)\n",
        "        batch_loss = loss.item()/batch_size\n",
        "        epoch_loss += batch_loss\n",
        "\n",
        "        # Propagate the loss value backward.\n",
        "        loss.backward()\n",
        "\n",
        "        # Trigger the optimizer to update once.\n",
        "        optimizer.step()\n",
        "        # End of one epoch\n",
        "\n",
        "    epoch_loss /= n_batches\n",
        "    print(f'epoch={epoch}, epoch_loss={epoch_loss}\\n')"
      ],
      "metadata": {
        "id": "wAI_3PVZ-x-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainRandn(model1)"
      ],
      "metadata": {
        "id": "_gRwZ8S_eeio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Optimization\n",
        "\n",
        "The problem of Elman RNN is its failing to capture long-range dependencies. If previous hidden state from previous tokens is irrelevant to the current state, it should not be included into the current output calculation. But Elman RNN doesn't control over hidden states. An enhanced architecture is called `Gated RNN` which selects hidden states to be included into calculation, therefore shows better performance on long-range language modeling tasks."
      ],
      "metadata": {
        "id": "pkdQo4oNMjm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gated RNN\n",
        "\n",
        "A `Gated Recurrent Neural Network (Gated RNN)` is a type of recurrent neural network that uses gating mechanisms to control the flow of hidden states. These gates effectively regulate the addition or removal of hidden states in the cell state, making the network capable of learning long-term dependencies in sequence data. Gated RNNs are particularly useful in addressing the `vanishing gradient` problem that is common in vanilla RNNs.\n",
        "\n",
        "Types of Gated RNNs\n",
        "\n",
        "1. Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM units include input, output, and forget gates. These gates determine what information should be retained or discarded at each step in the sequence, allowing the network to maintain a longer memory.\n",
        "\n",
        "* Input Gate\n",
        "\n",
        "> It decides which values from the input should be used to update the memory.\n",
        "\n",
        "<img src=\"https://github.com/annesjyu/NLP2/blob/main/lstm_input_gate.gif?raw=true\" width=350></img>\n",
        "\n",
        "* Forget Gate\n",
        "\n",
        "> It decides what information should be thrown away from the block.\n",
        "\n",
        "<img src=\"https://github.com/annesjyu/NLP2/blob/main/lstm_forget_gate.gif?raw=true\" width=350></img>\n",
        "\n",
        "* Cell State\n",
        "\n",
        "> It is a combination of input and forgot gate outputs. The current cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. It then gets pointwise added with input gate vector to get a new cell state.\n",
        "\n",
        "<img src=\"https://github.com/annesjyu/NLP2/blob/main/lstm_cell_state.gif?raw=true\" width=350></img>\n",
        "\n",
        "* Output Gate\n",
        "\n",
        "> It determines what the next hidden state should be.\n",
        "\n",
        "<img src=\"https://github.com/annesjyu/NLP2/blob/main/lstm_output_gate.gif?raw=true\" width=350></img>\n",
        "\n",
        "2. Gated Recurrent Unit (GRU)\n",
        "\n",
        "GRUs are a simpler variant of LSTMs. They combine the input and forget gates into a single \"update gate\" and also merge the cell state and hidden state, resulting in a more streamlined model that can perform on par with LSTM for certain tasks.\n",
        "\n",
        "* Update Gate\n",
        "\n",
        "> It helps the model decide how much of the past information (from previous time steps) needs to be passed along to the future.\n",
        "\n",
        "* Reset Gate\n",
        "\n",
        "> It decides how much of the past information to forget.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/annesjyu/NLP2/main/lstm_gated_rnn.webp\" width=350></img>\n",
        "\n",
        "#### Reference:\n",
        "\n",
        "Phi, M. (2020, June 28). Illustrated Guide to LSTM’s and GRU’s: A step by step explanation. Medium. https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
      ],
      "metadata": {
        "id": "u7QWGkNLSj0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mathematical Representation\n",
        "\n",
        "##### Gated RNN\n",
        "\n",
        "$\n",
        "h_t = h_{t-1} + λ(h_{t-1}, x_t) × F(U*x_t + W * h_{t-1} + b), \\\\\n",
        "y_t = V * h_t + c\n",
        "$\n",
        "\n",
        "Where,\n",
        "\n",
        "$λ(h_{t-1}, x_t)$ is the gating function, with a value in $[0,1]$. Furthur, $λ$ is context-dependent and it is usually a sigmoid function.\n",
        "\n",
        "##### LSTM\n",
        "\n",
        "Hochreiter and Schmidhuber describe the function as below,\n",
        "\n",
        "$\n",
        "h_t = μ(h_{t-1}, x_t)h_{t-1} + λ(h_{t-1}, x_t)F(h_{t-1}, x_t)\n",
        "$\n",
        "\n",
        "Where,\n",
        "\n",
        "$μ$ is another gating function.\n",
        "\n",
        "###### Reference:\n",
        "\n",
        "* Hochreiter, S., Schmidhuber, J. \"Long Short-Term Memory\". Neural Comput 1997; 9 (8): 1735–1780. doi: https://doi.org/10.1162/neco.1997.9.8.1735\n",
        "\n",
        "* Olah, C. \"Understanding LSTM Networks\". (August, 2015). https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n"
      ],
      "metadata": {
        "id": "uJbnduNmYZU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "kfSwinavbvwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedRNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_size,\n",
        "               hidden_size,\n",
        "               output_size,\n",
        "               num_layers=1):\n",
        "    '''\n",
        "    input_size: The number of expected features in the input x\n",
        "    hidden_size: The number of features in the hidden state h\n",
        "    output_size: The number of features in the output y\n",
        "    num_layers: The number of recurrent layers.\n",
        "    '''\n",
        "    super(SimpleRNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    # Gated Recurrent Network layer\n",
        "    self.gru = nn.GRU(input_size=self.input_size,\n",
        "                      hidden_size=self.hidden_size,\n",
        "                      num_layers=self.num_layers,\n",
        "                      batch_first=True) # it is true at the step of initialization\n",
        "    # Go through another linear layer to compute final output.\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Initialize the hidden state with zeros.\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "    hidden, _ = self.gru(x, h0)\n",
        "    # Pass the output of the last time step to the final classifier\n",
        "    return self.fc(hidden[:, -1, :])"
      ],
      "metadata": {
        "id": "R28kwaLhcGtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Excercise"
      ],
      "metadata": {
        "id": "50Xrjm6udtil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can you train random data on a GatedRNN model\n"
      ],
      "metadata": {
        "id": "ZKhBhMA7ddzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}